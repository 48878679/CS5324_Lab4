{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4, Group 4\n",
    "### Names: Hailey DeMark, Deborah Park, Karis Park\n",
    "### Student IDs: 48869449, 48878679, 48563429\n",
    "\n",
    "Link to DataSet: https://www.kaggle.com/datasets/muonneutrino/us-census-demographic-data/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Split, and Balance (1.5 points total)\n",
    "\n",
    "* [.5 points] (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric. (2) Remove any observations that having missing data. (3) Encode any string data as integers for now. (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable. \n",
    "\n",
    "* // answer goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned data: (72718, 36)\n",
      "Remaining columns: ['TractId', 'State', 'TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'VotingAgeCitizen', 'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty', 'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork', 'Unemployment']\n"
     ]
    }
   ],
   "source": [
    "# Load Data \n",
    "file = 'data.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# remove missing rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# convert string to integer\n",
    "categorical_columns = df.select_dtypes(include='object').columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# remove county\n",
    "df = df.drop(columns=['County'])\n",
    "\n",
    "# check\n",
    "print(\"Shape of cleaned data:\", df.shape)\n",
    "print(\"Remaining columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should balancing of the dataset be done for both the training and testing set? Explain.\n",
    "* Balancing of the dataset should only be done for the training set because we don't want to touch the testing set as much as possible. In a real-life scenario, we don't have access to the testing set, or future data, when training our model on the past data. So, we want to mimic these conditions as much as possible, leaving the testing data untouched until we're done training our model on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58174, 35) (14544, 35) (58174,) (14544,)\n",
      "Quantile thresholds: [ 6.2 16.4 31.7]\n",
      "Class distribution in training set:\n",
      "Class 0: 14596 instances\n",
      "Class 1: 14580 instances\n",
      "Class 2: 14476 instances\n",
      "Class 3: 14522 instances\n"
     ]
    }
   ],
   "source": [
    "# split features and target\n",
    "y = df['ChildPoverty'].values\n",
    "X = df.drop(columns=['ChildPoverty']).values\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# split 'childpoverty' into 4 quantiles\n",
    "quantiles = np.quantile(y_train, [0.25, 0.5, 0.75])\n",
    "\n",
    "def quant_poverty(val):\n",
    "    if val <= quantiles[0]:\n",
    "        return 0\n",
    "    elif val <= quantiles[1]:\n",
    "        return 1\n",
    "    elif val <= quantiles[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# Apply the function to the numpy array\n",
    "vectorized_quant = np.vectorize(quant_poverty)\n",
    "y_train_classes = vectorized_quant(y_train)\n",
    "y_test_classes = vectorized_quant(y_test)\n",
    "\n",
    "# One-hot encode the targets\n",
    "y_train_dummy = np.eye(4)[y_train_classes]\n",
    "y_test_dummy = np.eye(4)[y_test_classes]\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Quantile thresholds:\", quantiles)\n",
    "print(\"Class distribution in training set:\")\n",
    "unique, counts = np.unique(y_train_classes, return_counts=True)\n",
    "for class_label, count in zip(unique, counts):\n",
    "    print(f\"Class {class_label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and Initial Modeling (2.5 points total)\n",
    "\n",
    "You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). It is recommended to use sigmoids throughout the network, but not required.\n",
    "\n",
    "* [.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "\n",
    "* [.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  \n",
    "\n",
    "* [.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "\n",
    "* [1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "    * Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment.\n",
    "We chose a customer segmentation dataset with 8,068 entries. It includes details about customers, such as their gender, age, marital status, education, job, work experience, spending habits, and family size. The goal is to predict which of four customer groups (A, B, C, or D) a customer belongs to based on their information. This dataset would be helpful for businesses to understand their customers and target audience better. Additionally, companies can use the results to send personalized offers, improve customer service, and make better marketing plans. This model would be deployed mostly for online-use because it's important for companies to keep up with the trends and preferences of their target audience and general customers to make the right choices in marketing and keep up sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (5 points total)\n",
    "\n",
    "* [1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "    * Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs.\n",
    "\n",
    "* [1 points] Repeat the previous step, adding support for a fourth layer.\n",
    "\n",
    "* [1 points] Repeat the previous step, adding support for a fifth layer. \n",
    "\n",
    "* [2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (choose either RMSProp or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)\n",
    "\n",
    "5000 level student: You have free reign to provide additional analyses.\n",
    "One idea (required for 7000 level students):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance compared to other methods.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
